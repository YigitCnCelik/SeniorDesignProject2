{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PhmkYmjCTuko"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\ALFA\n",
            "[nltk_data]     V1\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\ALFA\n",
            "[nltk_data]     V1\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlUyFMYmsq-b",
        "outputId": "a738b461-9409-485b-da17-5f14b455e219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Classifier: Accuracy = 0.88\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from snowballstemmer import TurkishStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text_turkish(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text, language='turkish')  # Tokenize in Turkish\n",
        "    stop_words = set(stopwords.words('turkish'))  # Turkish stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    stemmer = TurkishStemmer()  # Turkish stemmer\n",
        "    tokens = [stemmer.stemWord(word) for word in tokens]  # Stemming\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load the dataset (adjust the path as needed)\n",
        "data_path = 'Data.xlsx'  # Replace with your file path\n",
        "data = pd.read_excel(data_path, sheet_name='Data')\n",
        "\n",
        "# Preprocess the dataset\n",
        "data['Clean_Comment_Turkish'] = data['Comment'].apply(preprocess_text_turkish)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['Clean_Comment_Turkish'])\n",
        "y = data['Topic']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier: Accuracy = {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVT8kzPCmKPC",
        "outputId": "4f44f698-4c90-4699-8c89-225d7a825fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ALFA  V1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\ALFA  V1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From c:\\Users\\ALFA  V1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\ALFA  V1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "681/681 [==============================] - 1s 876us/step - loss: 1.5468 - accuracy: 0.4518\n",
            "Epoch 2/10\n",
            "681/681 [==============================] - 1s 879us/step - loss: 0.6870 - accuracy: 0.8025\n",
            "Epoch 3/10\n",
            "681/681 [==============================] - 1s 893us/step - loss: 0.4670 - accuracy: 0.8667\n",
            "Epoch 4/10\n",
            "681/681 [==============================] - 1s 876us/step - loss: 0.3834 - accuracy: 0.8895\n",
            "Epoch 5/10\n",
            "681/681 [==============================] - 1s 872us/step - loss: 0.3321 - accuracy: 0.9039\n",
            "Epoch 6/10\n",
            "681/681 [==============================] - 1s 889us/step - loss: 0.2944 - accuracy: 0.9124\n",
            "Epoch 7/10\n",
            "681/681 [==============================] - 1s 889us/step - loss: 0.2717 - accuracy: 0.9184\n",
            "Epoch 8/10\n",
            "681/681 [==============================] - 1s 882us/step - loss: 0.2535 - accuracy: 0.9213\n",
            "Epoch 9/10\n",
            "681/681 [==============================] - 1s 881us/step - loss: 0.2365 - accuracy: 0.9243\n",
            "Epoch 10/10\n",
            "681/681 [==============================] - 1s 883us/step - loss: 0.2254 - accuracy: 0.9281\n",
            "Neural Network Accuracy: 0.88\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'data' is your DataFrame with raw text and labels\n",
        "X_raw = data['Comment']  # Replace with your column name\n",
        "y_raw = data['Topic']  # Replace with your column name\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_raw)\n",
        "X_seq = tokenizer.texts_to_sequences(X_raw)\n",
        "X_padded = pad_sequences(X_seq, padding='post', maxlen=50)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_raw)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=50),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(y_categorical.shape[1], activation='softmax')  # Use 'sigmoid' for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Use 'binary_crossentropy' for binary\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Neural Network Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mz3MGSJmc-T",
        "outputId": "259f33b2-d0e6-4172-da3f-33434dcff364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "681/681 [==============================] - 1s 889us/step - loss: 1.5848 - accuracy: 0.4467\n",
            "Epoch 2/10\n",
            "681/681 [==============================] - 1s 888us/step - loss: 0.6979 - accuracy: 0.8004\n",
            "Epoch 3/10\n",
            "681/681 [==============================] - 1s 882us/step - loss: 0.4717 - accuracy: 0.8655\n",
            "Epoch 4/10\n",
            "681/681 [==============================] - 1s 884us/step - loss: 0.3874 - accuracy: 0.8892\n",
            "Epoch 5/10\n",
            "681/681 [==============================] - 1s 884us/step - loss: 0.3419 - accuracy: 0.8979\n",
            "Epoch 6/10\n",
            "681/681 [==============================] - 1s 904us/step - loss: 0.3111 - accuracy: 0.9063\n",
            "Epoch 7/10\n",
            "681/681 [==============================] - 1s 899us/step - loss: 0.2893 - accuracy: 0.9108\n",
            "Epoch 8/10\n",
            "681/681 [==============================] - 1s 899us/step - loss: 0.2688 - accuracy: 0.9166\n",
            "Epoch 9/10\n",
            "681/681 [==============================] - 1s 908us/step - loss: 0.2538 - accuracy: 0.9198\n",
            "Epoch 10/10\n",
            "681/681 [==============================] - 1s 900us/step - loss: 0.2411 - accuracy: 0.9226\n",
            "bu telefon çok güzel\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "ağırlık: 0.31%\n",
            "hediye: 0.34%\n",
            "kalite: 85.77%\n",
            "kalıp/boy/ölçü: 4.16%\n",
            "kullanımı kolay: 1.98%\n",
            "renk: 0.30%\n",
            "ses: 0.11%\n",
            "çeyiz: 7.04%\n",
            "The probable category of the comment: kalite\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Türkçe için metin ön işleme adımları\n",
        "def preprocess_text_turkish(text):\n",
        "    text = text.lower()  # Küçük harfe dönüştürme\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Noktalama işaretlerini kaldırma\n",
        "    tokens = word_tokenize(text, language='turkish')  # Tokenleme için Türkçe\n",
        "    stop_words = set(stopwords.words('turkish'))  # Türkçe stop-word'leri alma\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Stop-word'leri kaldırma\n",
        "    stemmer = TurkishStemmer()  # Türkçe kök çıkarma\n",
        "    tokens = [stemmer.stemWord(word) for word in tokens]  # Kök çıkarma\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Data.xlsx'\n",
        "data = pd.read_excel(file_path, sheet_name='Data')\n",
        "\n",
        "# Preprocess the dataset\n",
        "data['Clean_Comment_Turkish'] = data['Comment'].apply(preprocess_text_turkish)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data['Clean_Comment_Turkish'])\n",
        "X_seq = tokenizer.texts_to_sequences(data['Clean_Comment_Turkish'])\n",
        "X_padded = pad_sequences(X_seq, padding='post', maxlen=50)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(data['Topic'])\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=50),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(y_categorical.shape[1], activation='softmax')  # Use 'sigmoid' for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Use 'binary_crossentropy' for binary\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "import tkinter as tk\n",
        "def on_enter_pressed():\n",
        "    global new_comment\n",
        "    new_comment = entry.get()\n",
        "\n",
        "    root.destroy()\n",
        "\n",
        "\n",
        "# Ana pencereyi oluştur\n",
        "root = tk.Tk()\n",
        "root.title(\"Yorum Girişi\")\n",
        "\n",
        "# Etiket (Label) oluştur\n",
        "label = tk.Label(root, text=\"Yorumunuzu Girin:\")\n",
        "label.pack(pady=10)\n",
        "\n",
        "# Metin giriş kutusu (Entry) oluştur\n",
        "entry = tk.Entry(root, width=40)\n",
        "entry.pack(pady=15)\n",
        "\n",
        "# Enter tuşuna basıldığında fonksiyonu çağır\n",
        "entry.bind('<Return>', lambda event=None: on_enter_pressed())\n",
        "\n",
        "# Buton oluştur\n",
        "button = tk.Button(root, text=\"Gönder\", command=on_enter_pressed)\n",
        "button.pack(pady=10)\n",
        "\n",
        "# Pencereyi başlat\n",
        "root.mainloop()\n",
        "\n",
        "print(new_comment)\n",
        "\n",
        "# Preprocess the new comment\n",
        "new_comment_clean = preprocess_text_turkish(new_comment)\n",
        "\n",
        "# Tokenize and pad the new comment\n",
        "new_comment_seq = tokenizer.texts_to_sequences([new_comment_clean])\n",
        "new_comment_padded = pad_sequences(new_comment_seq, padding='post', maxlen=50)\n",
        "\n",
        "# Predict with the model\n",
        "probabilities = model.predict(new_comment_padded)[0]\n",
        "\n",
        "# Display probabilities for each category\n",
        "all_categories = label_encoder.classes_\n",
        "category_list = []\n",
        "probabilities_list = []\n",
        "for i, category in enumerate(all_categories):\n",
        "\n",
        "    print(f\"{category}: {probabilities[i]*100:.2f}%\")\n",
        "    category_list.append(category)\n",
        "    probabilities_list.append(probabilities[i])\n",
        "\n",
        "\n",
        "\n",
        "# Find the most likely category\n",
        "most_likely_category = all_categories[np.argmax(probabilities)]\n",
        "print(f\"The probable category of the comment: {most_likely_category}\")\n",
        "\n",
        "import tkinter as tk\n",
        "\n",
        "def show_probabilities(probabilities, all_categories, most_likely_category):\n",
        "    root = tk.Tk()\n",
        "    root.title(\"Category Probabilities\")\n",
        "\n",
        "    # Create a text widget to display probabilities\n",
        "    text_widget = tk.Text(root, height=len(all_categories) + 2, width=80)\n",
        "    text_widget.pack()\n",
        "\n",
        "    # Display probabilities for each category\n",
        "    for i, category in enumerate(all_categories):\n",
        "        probability_percentage = probabilities[i] * 100\n",
        "        text_widget.insert(tk.END, f\"{category}: {probability_percentage:.2f}%\\n\")\n",
        "\n",
        "    # Display the most likely category\n",
        "    text_widget.insert(tk.END, f\"\\nThe probable category of the comment: {most_likely_category}\")\n",
        "\n",
        "    root.mainloop()\n",
        "\n",
        "# Kullanım örneği:\n",
        "# Burada probabilities, all_categories ve most_likely_category değişkenlerinizi kullanmalısınız.\n",
        "# Örnek değerlerle kullanıyormuş gibi düşünelim:\n",
        "example_probabilities = [0.3, 0.6, 0.1]\n",
        "example_all_categories = ['Category1', 'Category2', 'Category3']\n",
        "example_most_likely_category = 'Category2'\n",
        "\n",
        "show_probabilities(probabilities_list, category_list, most_likely_category)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7GVEhntxqGE",
        "outputId": "b968de65-41cc-4b9b-ff88-4be55c68b0cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "amazon_reviews_categorized.xlsx dosyası başarıyla oluşturuldu ve kaydedildi.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-7c0342407633>:58: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
            "  writer.save()\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import contextlib\n",
        "from openpyxl import load_workbook\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# İlk attığınız kodda tanımlanan modeli ve diğer gerekli nesneleri yükleme\n",
        "# model, tokenizer, label_encoder gibi nesnelerin tanımlandığı varsayılıyor\n",
        "\n",
        "# Input ve Output path'lerini belirleme\n",
        "input_path = 'amazon_reviews.xlsx'\n",
        "output_path = 'amazon_reviews_categorized.xlsx'\n",
        "\n",
        "# Input Excel dosyasını okuma (başlık yoksa)\n",
        "data = pd.read_excel(input_path, header=None)\n",
        "\n",
        "# Yorumların bulunduğu sütunun indeksi (örneğin, ilk sütun için 0)\n",
        "comment_column_index = 0\n",
        "\n",
        "# Yorumları modelle etiketleme\n",
        "def label_comment(comment):\n",
        "    new_comment_clean = preprocess_text_turkish(comment)\n",
        "    new_comment_seq = tokenizer.texts_to_sequences([new_comment_clean])\n",
        "    new_comment_padded = pad_sequences(new_comment_seq, padding='post', maxlen=50)\n",
        "    probabilities = model.predict(new_comment_padded)[0]\n",
        "\n",
        "    # Olasılığı %35'un üstünde olan kategorileri bulma\n",
        "    threshold = 0.35\n",
        "    likely_categories = [label_encoder.classes_[i] for i, prob in enumerate(probabilities) if prob > threshold]\n",
        "\n",
        "    # Kategorilere ait olasılıkları alarak yüzdeye çevirme\n",
        "    category_probabilities = [f\"{prob * 100:.2f}%\" for i, prob in enumerate(probabilities) if label_encoder.classes_[i] in likely_categories]\n",
        "\n",
        "    return likely_categories, category_probabilities\n",
        "\n",
        "# Her yorum için etiket ekleme\n",
        "data['Predicted_Topics'], data['Probabilities'] = zip(*data[comment_column_index].apply(label_comment))\n",
        "\n",
        "# Excel'e yazdırma işlemi\n",
        "with open(os.devnull, 'w') as nullfile:\n",
        "    with contextlib.redirect_stdout(nullfile):\n",
        "        writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
        "        for label in data['Predicted_Topics'].explode().unique():\n",
        "            if isinstance(label, str):  # Check if label is already a string\n",
        "                cleaned_label = label.replace(\"/\", \"_\")  # Replace \"/\" with \"_\"\n",
        "            else:\n",
        "                cleaned_label = str(label)  # Convert non-string types to strings and then replace\n",
        "                cleaned_label = cleaned_label.replace(\"/\", \"_\")  # Replace \"/\" with \"_\"\n",
        "\n",
        "            labeled_data = data[data['Predicted_Topics'].apply(lambda x: label in x)]\n",
        "            labeled_data.to_excel(writer, sheet_name=cleaned_label, index=False, columns=[comment_column_index])\n",
        "\n",
        "        # 'All' kısmında Comments ve Predicted Topics kolonları olsun\n",
        "        all_data = data.explode('Predicted_Topics')[[comment_column_index, 'Predicted_Topics', 'Probabilities']]\n",
        "        all_data.to_excel(writer, sheet_name='All', index=False, header=['Comments', 'Predicted Topics', 'Probabilities'])\n",
        "\n",
        "        # Excel dosyasını kaydetme ve kapatma\n",
        "        writer.save()\n",
        "\n",
        "print(f\"{output_path} dosyası başarıyla oluşturuldu ve kaydedildi.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
